\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{url}
\usepackage{amsmath}

% CleverRef Setup
\usepackage[capitalize,noabbrev]{cleveref}

\title{Parallel Sparse Convolution for Deep Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Ravi Teja Mullapudi 
  \And
  Prashanth Menon
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Performing inference on high-resolution video and image data using
state-of-the-art deep vision models is computationally demanding.  At the
heart of deep learning models for computer vision is the convolution
operation which is a 3D filtering operation on high-dimensional features.
Improving the performance of convolution layers continues to be a key aspect
of enabling computer vision research. There have been several efforts to
improve the efficiency by switching to a different domain (FFT, Winograd).
However, these transformations do not account for the possibility of only
needing to perform the convolution sparsely across the image (the domain
transformation significantly reduces the sparsity). Given the high
redundancy between video frames or the sparse signal generated by sensors
like LIDAR one can determine the sparse spatial locations to perform the
convolution. In this project, we demonstrate that a simple parallel and
locality-aware implementation of sparse convolution is indeed competitive
with highly-tuned vendor implementations of dense convolutions which operate
in a transformed domain.
\end{abstract}

\bibliographystyle{abbrv}
\nocite{*}
{\bibliography{report}}

\end{document}
